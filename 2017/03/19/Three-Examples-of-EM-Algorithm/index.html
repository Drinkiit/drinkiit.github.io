<!doctype html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="The EM algorithm formalizes a fairly old approach to handling missing data: first replace missing data by estimated values, then estimates the model parameters, and perhaps, repeat these two steps sev">
<meta property="og:type" content="article">
<meta property="og:title" content="Three Examples of EM Algorithm">
<meta property="og:url" content="http://yoursite.com/2017/03/19/Three-Examples-of-EM-Algorithm/index.html">
<meta property="og:site_name" content="Yida's Blog">
<meta property="og:description" content="The EM algorithm formalizes a fairly old approach to handling missing data: first replace missing data by estimated values, then estimates the model parameters, and perhaps, repeat these two steps sev">
<meta property="og:image" content="http://yoursite.com/2017/03/19/Three-Examples-of-EM-Algorithm/motorette.png">
<meta property="og:updated_time" content="2017-03-23T21:37:20.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Three Examples of EM Algorithm">
<meta name="twitter:description" content="The EM algorithm formalizes a fairly old approach to handling missing data: first replace missing data by estimated values, then estimates the model parameters, and perhaps, repeat these two steps sev">
<meta name="twitter:image" content="http://yoursite.com/2017/03/19/Three-Examples-of-EM-Algorithm/motorette.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/03/19/Three-Examples-of-EM-Algorithm/"/>





  <title> Three Examples of EM Algorithm | Yida's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-91889551-1', 'auto');
  ga('send', 'pageview');
</script>











  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Yida's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/03/19/Three-Examples-of-EM-Algorithm/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Yida Yin">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/smiling_man.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Yida's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Yida's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Three Examples of EM Algorithm
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-19T19:41:34-05:00">
                2017-03-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/03/19/Three-Examples-of-EM-Algorithm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/03/19/Three-Examples-of-EM-Algorithm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>The EM algorithm formalizes a fairly old approach to handling missing data: first replace missing data by estimated values, then estimates the model parameters, and perhaps, repeat these two steps several times.</p>
<a id="more"></a>
<h3 id="Example-1-Genetic-Linkage-Model-Rao-1973"><a href="#Example-1-Genetic-Linkage-Model-Rao-1973" class="headerlink" title="Example 1. Genetic Linkage Model (Rao, 1973)"></a>Example 1. Genetic Linkage Model (Rao, 1973)</h3><h4 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h4><p>Suppose the 197 animals ($Y$) are distributed into four categories as follows:<br>$$Y = (y_1, y_2, y_3, y_4) = (125, 18, 20, 34)$$<br>with cell probabilities .<br>$$\big( \frac{1}{2}+\frac{\theta}{4}, \frac{1}{4}(1-\theta), \frac{1}{4}(1-\theta), \frac{\theta}{4} \big)$$<br>The observed data likelihood:<br>$$g(Y\ |\ \theta) \propto \big(\frac{1}{2}+\frac{\theta}{4}\big)^{y_1} \big(\frac{1}{4}(1-\theta)\big)^{y_2+y_3} \big(\frac{\theta}{4}\big)^{y_4}$$</p>
<p>Augment the observed data by splitting the first cell into two cells with probabilities $\frac{1}{2}$ and $\frac{\theta}{4}$. The augmented data are given by<br>$$(x_1,x_2,x_3,x_4,x_5)$$<br>such that<br>
$$
\begin{aligned}
x_1+x_2 &= y_1=125  \\
    x_3 &= y_2=18  \\
    x_4 &= y_3=20  \\
    x_5 &= y_4=34
\end{aligned}
$$
<br>The “missing data” $Z(=x_2)$ is like a coin toss for which category each observation of $y_1$ gets split into:<br>$$Z \sim Binomial(125, p), \text{where } p=\frac{\theta/4}{1/2 + \theta/4} = \frac{\theta}{2+\theta}$$<br>The augmented data likelihood is now simpler:<br>$$f(X, Z\ |\ \theta) \propto (1-\theta)^{x_3+x_4} \theta^{x_2+x_5}$$<br><strong>E-step. </strong><br>Now we compute the Q function (under a flat prior):<br>
$$
\begin{aligned}
Q(\theta, \theta^{old}) = E_{Z | X, \theta^{old}} \log f(X\ |\ \theta) &= \log \theta \big(E_{Z | Y, \theta^{old}}[x_2] + x_5 \big) + (x_3+x_4)\log (1-\theta)  \\
&= \log \theta \big(125 \times \frac{\theta^{old}}{2+\theta^{old}} + 34 \big) + (18+20)\log (1-\theta)
\end{aligned}
$$
<br>where the second equality follows from the mean of a binomial distribution.<br><strong>M-step. </strong><br>
$$
\begin{aligned}
\frac{\partial Q(\theta, \theta^{old})}{\partial \theta} \bigg|_{\theta=\theta^{old}} = 0 &\Rightarrow \big(125 \times \frac{\theta^{old}}{2+\theta^{old}} + 34 \big) \times \frac{1}{\theta} - \frac{38}{1-\theta}=0  \\
&\Rightarrow \theta = \frac{125 \times \frac{\theta^{old}}{2+\theta^{old}} + 34}{125 \times \frac{\theta^{old}}{2+\theta^{old}} + 34 + 38}  \text{ (recursive formula) }
\end{aligned}
$$
</p>
<h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><p>Starting at $\theta^0 = 0.5$, the EM algorithm converges to $\theta^*=0.6268$ (the observed posterior mode) after a few iterations. One might ask can we take the derivate of the observed data likelihood (original data likelihood) and set it to zero to get the answer? The answer is yes. The derivate is given as follow:<br>$$\frac{d}{d \theta} \log g(\theta | Y) = \frac{125}{2+\theta} - \frac{38}{1-\theta} + \frac{34}{\theta} = 0$$<br>It is easy to see that $\theta^*$ satisfies this equation (see M-step). Which means, in this case, EM algorithm yields the same result as the MLE.  </p>
<p>Another question is how to estimate the variance of $\theta^*$? Statisticians are not satisfied with only the estimaed value of $\mu$, they also want to know the variance of the estimate. In general, there are two kind of methods: (1) Parametric Bootstrap (2) Louis’ Method [2].<br>(1) Parametric Bootstrap:  </p>
<ul>
<li>Generate random Y based on $\mu^*$   </li>
<li>Run EM algorithm on each bootstrap sample   </li>
<li>Calculate the bootstrap variance of the estimated $\mu$    </li>
</ul>
<p>(2) Louis’ Method [2] TODO    </p>
<h3 id="Example-2-Linear-Regression-with-Right-Censored-Data"><a href="#Example-2-Linear-Regression-with-Right-Censored-Data" class="headerlink" title="Example 2. Linear Regression with Right-Censored Data"></a>Example 2. Linear Regression with Right-Censored Data</h3><p>We consider the motorette data found in Schmee and Hahn (1979). Ten motorettes were tested at each of the four temperatures: 150C, 170C, 190C and 220C. The time to failure in hours is given in Table below.   </p>
<p><img src="/2017/03/19/Three-Examples-of-EM-Algorithm/motorette.png" alt="motorette_data" style="width: 60%;"></p>
<p>A star indicates that a motorette was taken off the study without failing at the event time indicated. For these data, we will fit the model:<br>$$t_i = \beta_0 + \beta_1 \nu_i + \sigma \epsilon_i$$   <br>where $\epsilon_i \sim N(0,1)$, $\nu_i=1000/(\text{temperature} + 273.2)$ and $t_i=\log_{10}$ ($i$th failure time).<br>Reorder the data so that the first $k$ observations are uncensored (i.e. a failure is observed at $t_i$) and the remaining $n-k$ are censored ($c_i$ denotes a censored event time). The log-augmented posterior (under a flat prior) is given (up to a constant by):<br>$$-n\log\sigma - \sum_{i=1}^k(t_i-\beta_0-\beta_1\nu_i)^2/2\sigma^2 - \sum_{i=k+1}^{n}(Z_i-\beta_0-\beta_1\nu_i)^2/2\sigma^2$$   <br>where $Z_j$ is the (unobserved) failure time for case $j$. The conditional predictive distribution $p(Z_i\ |\ \beta_0, \beta_1, \sigma, c_i)$ is the conditional normal distribution, conditional on the fact that the unobserved failure time $Z_j$ is greater than $c_j$.<br><strong>E-step .</strong> Compute:<br>
$$
\begin{aligned}
-n\log\sigma &- \frac{1}{2\sigma^2}\sum_{i=1}^{k}(t_i-\beta_0-\beta_1\nu_i)^2 \\
&- \frac{1}{2\sigma^2}\sum_{i=k+1}^{n}\bigg[  E[Z_i^2\ |\ \beta_0^{old}, \beta_1^{old}, \sigma^{old}, Z_i > c_i] - 2(\beta_0+\beta_1)E[Z_i\ |\ \beta_0^{old}, \beta_1^{old}, \sigma^{old}, Z_i > c_i]+(\beta_0+\beta_1\nu_i)^2 \bigg]
\end{aligned}
$$
<br>Note that:<br>$$ E[Z_i^2\ |\ \beta_0^{old}, \beta_1^{old}, \sigma^{old}, Z_i > c_i] = \mu_i^2 + \sigma^2 + \sigma (c_i+\mu_i) H\bigg(\frac{c_i-\mu_i}{\sigma} \bigg)$$<br>and<br>$$ E[Z_i\ |\ \beta_0^{old}, \beta_1^{old}, \sigma^{old}, Z_i > c_i] = \mu_i + \sigma H\bigg(\frac{c_i-\mu_i}{\sigma} \bigg) $$  <br>where $\mu_i = \beta_0+\beta_1\nu_i$, $H(x)=\phi(x)/(1-\Phi(x))$, and $\phi(x)$ and $\Phi(x)$ are the density and cdf of the standard normal distribution.<br><strong>M-step .</strong><br>
$$
\begin{aligned}
\frac{\partial Q}{\partial\beta_0} &= 0 \rightarrow \sum_{i=1}{m}(t_i-\beta_0-\beta_1\nu_i) + \sum_{i=k+1}^{n}\big[ E(Z_i)-\beta_0-\beta_1\nu_i \big] = 0  \\
\frac{\partial Q}{\partial\beta_1} &= 0 \rightarrow \sum_{i=1}{m}\nu_i(t_i-\beta_0-\beta_1\nu_i) + \sum_{i=k+1}^{n}\nu_i\big[ E(Z_i)-\beta_0-\beta_1\nu_i \big] = 0  \\
\frac{\partial Q}{\partial\sigma^2} &= 0 \rightarrow \frac{\sum_{i=1}^k(t_i-\beta_0-\beta_1\nu_i)^2}{\sigma^4} + \frac{\sum_{i=k+1}^n \big[ E(Z_i)^2-2(\beta_0+\beta_1\nu_i)E(Z_i)+(\beta_0+\beta_1\nu_i)^2 \big]}{\sigma^4} - \frac{n}{\sigma^4} = 0
\end{aligned}
$$
<br>To obtain $\beta^{new}$, replace $c_j$ by $E[Z_j\ |\ \beta_0^{old}, \beta_1^{old},\sigma^{old}, Z_j>c_j]$ and apply least squares. To obtain $\sigma_{new}^2$, compute:<br>$$\sigma_{new}^2 = \frac{\sum_{j=1}^k(t_j-\mu_j^{old})^2}{n} + \sigma_{old}^2 \frac{\sum_{j=k+1}^n \big[ 1+\big(\frac{c_j-\mu_j^{old}}{\sigma_{old}}H\big( \frac{c_j-\mu_j^{old}}{\sigma_{old}} \big) \big]}{n}$$  <br>where $\mu_j^{old} = \beta_0^{old}+\beta_1^{old}\nu_j$</p>
<h3 id="Example-3-Normal-distribution-with-unknown-mean-and-variance-and-partially-conjugate-prior-distribution"><a href="#Example-3-Normal-distribution-with-unknown-mean-and-variance-and-partially-conjugate-prior-distribution" class="headerlink" title="Example 3. Normal distribution with unknown mean and variance and partially conjugate prior distribution"></a>Example 3. Normal distribution with unknown mean and variance and partially conjugate prior distribution</h3><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>This example comes from Gelman’s book[1 Page 322], which shows the application of EM algorithm in Bayesian theory. Bayesian computation revolves around two steps: computation of the posterior distribution, $p(\theta\ |\ y)$, and computation of the posterior predictive distribution, $p(\tilde{y}\ |\ y)$. In problems with many parameters, it is very often we are only interested in a $subset$ of the parameters; we use the notation $\theta = (\gamma, \phi)$ and suppose we are interested in approximating $p(\phi \ |\ y)$. Then the EM algorithm can be viewed as an iterative method for finding the mode of the marginal posterior density, $p(\phi \ |\ y)$.   </p>
<h4 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem"></a>Problem</h4><p>Suppose we weigh an object on a scale $n$ times, and the weighings, $y_1, \dots, y_n$, are assumed independent with a $N(\mu, \sigma^2)$ distribution, where $\mu$ is the true weight of the object. And we assume a $N(\mu_0, \tau_0^2)$ prior distribution on $\mu$ (with $\mu_0$ and $\tau_0$ known) and a non-informative uniform prior distribution on $\log \sigma$ (that is $p(\sigma) = 1/\sigma$); these form a partially conjugate joint prior distribution.    </p>
<h4 id="Discussion-1"><a href="#Discussion-1" class="headerlink" title="Discussion"></a>Discussion</h4><p>Because the model is not fully conjugate, there is no standard form for the joint posterior distribution of $(\mu, \sigma)$ and no closed-form expression for the marginal posterior density $\mu$. In this problem, since there are only two parameters we can easily do a grid search over $\mu$ and $\sigma$ and then create a simple contour plot. But if we are only interested in $\mu$, we can, however, think of $\mu$ as the parameter in our problem and $\sigma$ as missing data. EM can therefore be used to find the marginal posterior mode of $\mu$, averaging over $\sigma$; that is, $(\mu, \sigma)$ corresponds to $(\phi, \gamma)$ in the general notation.    </p>
<h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><p>Model:<br>
$$
\begin{aligned}
& y\ |\ \mu, \sigma^2 \sim N(\mu, \sigma^2)   \\
& \mu\ |\ \mu_0, \tau_0^2 \sim N(\mu_0, \tau_0^2)   \\
& \log \sigma \propto 1
\end{aligned}
$$
</p>
<p>Write down the joint distribution:<br>
$$
\begin{aligned}
p(y, \mu, \sigma) &= p(y\ |\ \mu, \sigma) \times p(\mu\ |\ \mu_0, \tau_0) \times p(\sigma)   \\
                  &= \prod_{i=1}^n  \frac{1}{\sqrt{2\pi} \sigma} exp\big({-\frac{1}{2\sigma^2}(y_i-\mu)^2}\big) \times \frac{1}{\sqrt{2\pi} \tau_0} exp\big({-\frac{1}{2\tau_0^2}(\mu-\mu_0)^2}\big) \times \frac{1}{\sigma}
\end{aligned}
$$
</p>
<p>Posterior distribution (ignoring terms that do not depend on $\mu$ or $\sigma^2$):<br>$$p(\mu, \sigma \ |\ y) \propto \big(\frac{1}{\sigma}\big)^{(n+1)} \prod_{i=1}^n exp\big({-\frac{1}{2\sigma^2}(y_i-\mu)^2}\big) \times exp\big({-\frac{1}{2\tau_0^2}(\mu-\mu_0)^2}\big)$$</p>
<p>Joint log posterior density:<br>
$$\log p(\mu, \sigma \ |\ y) = -\frac{1}{2\tau_0^2}(\mu-\mu_0)^2 - (n+1)\log \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i-\mu)^2 + constant$$
</p>
<p><strong>E-step.</strong> For the E-step of the EM algorithm, we must determine the expectation of the joint log posterior density, averaging over $\sigma$ and conditional on the current guess $\mu^{old}$ and y:  </p>

$$E_{\sigma\ |\ \mu_{old},y} \log p(\mu, \sigma\ |\ y) = -\frac{1}{2\tau_0^2}(\mu-\mu_0)^2 - (n+1) E_{\sigma\ |\ \mu_{old},y} [\log \sigma] - \frac{1}{2}E_{\sigma\ |\ \mu_{old},y} \big[ \frac{1}{\sigma^2} \big] \sum_{i=1}^n (y_i-\mu)^2 + constant$$   

<p>Actually, we need evaluate only the second expectation $E_{\sigma\ |\ \mu_{old},y} [ \frac{1}{\sigma^2} ]$, because the first expectation does not involve $\mu$ and thus will not affect the M-step. The expectation can be evaluated by noting that, given $\mu$, the posterior distribution of $\sigma^2$ is scaled inverse-chi-squared:  </p>

$$\sigma^2\ | \ \mu, y \sim Inv-\chi^2\big(n, \frac{1}{n}\sum_{i=1}^n(y_i-\mu)^2\big)$$

<p>Then the conditional posterior distribution of $\frac{1}{\sigma^2}$ is a scaled $\chi^2$, and<br>
$$E_{\sigma\ |\ \mu_{old},y} \big[ \frac{1}{\sigma^2} \ \big|\ \mu_{old}, y \big] = \bigg( \frac{1}{n} \sum_{i=1}^n(y_i-\mu^{old})^2 \bigg)^{-1}$$
<br>We can then re-express the expectation as:<br>
$$E_{\sigma\ |\ \mu_{old},y} \log p(\mu, \sigma\ |\ y) = -\frac{1}{2\tau_0^2}(\mu-\mu_0)^2 - \frac{1}{2}\bigg( \frac{1}{n} \sum_{i=1}^n(y_i-\mu^{old})^2 \bigg)^{-1} \sum_{i=1}^n (y_i-\mu)^2 + constant$$  
<br><strong>M-step.</strong>  For the M-step, we must find the $\mu$ that maximizes the above expression. The task is straightforward, simply take the derivate and set it to 0, then we have:<br>
$$\mu^{new} = \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\frac{1}{n}\sum_{i=1}^n(y_i-\mu^{old})^2}\bar{y} }{\frac{1}{\tau_0^2} + \frac{n}{\frac{1}{n}\sum_{i=1}^n(y_i-\mu^{old})^2} }$$   
<br>If we iterate this computation, $\mu$ converges to the marginal mode of $p(\mu\ |\ y)$.</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/18/isotonic-regression/" rel="next" title="Isotonic Regression">
                <i class="fa fa-chevron-left"></i> Isotonic Regression
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/03/24/What-I-Learned-from-Peter-Norvig/" rel="prev" title="What I Learned from Peter Norvig">
                What I Learned from Peter Norvig <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/smiling_man.jpg"
               alt="Yida Yin" />
          <p class="site-author-name" itemprop="name">Yida Yin</p>
          <p class="site-description motion-element" itemprop="description">There are three kinds of lies: lies, damned lies and statistics</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://yidayin.me/" target="_blank" title="AboutMe">
                  
                    <i class="fa fa-fw fa-hand-spock-o"></i>
                  
                  AboutMe
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Example-1-Genetic-Linkage-Model-Rao-1973"><span class="nav-number">1.</span> <span class="nav-text">Example 1. Genetic Linkage Model (Rao, 1973)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem"><span class="nav-number">1.1.</span> <span class="nav-text">Problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discussion"><span class="nav-number">1.2.</span> <span class="nav-text">Discussion</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Example-2-Linear-Regression-with-Right-Censored-Data"><span class="nav-number">2.</span> <span class="nav-text">Example 2. Linear Regression with Right-Censored Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Example-3-Normal-distribution-with-unknown-mean-and-variance-and-partially-conjugate-prior-distribution"><span class="nav-number">3.</span> <span class="nav-text">Example 3. Normal distribution with unknown mean and variance and partially conjugate prior distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Introduction"><span class="nav-number">3.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem-1"><span class="nav-number">3.2.</span> <span class="nav-text">Problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discussion-1"><span class="nav-number">3.3.</span> <span class="nav-text">Discussion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Solution"><span class="nav-number">3.4.</span> <span class="nav-text">Solution</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yida Yin</span>
</div>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'drinkiit';
      var disqus_identifier = '2017/03/19/Three-Examples-of-EM-Algorithm/';

      var disqus_title = "Three Examples of EM Algorithm";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      
        var disqus_config = function () {
            this.page.url = disqus_url;
            this.page.identifier = disqus_identifier;
            this.page.title = disqus_title;
        };
        run_disqus_script('embed.js');
      

    </script>
  










  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


</body>
</html>
